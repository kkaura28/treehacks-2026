{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222b6875",
   "metadata": {},
   "source": [
    "# Video Depth Estimation with Depth Anything V3\n",
    "Sample every Nth frame from a video, run depth estimation, and save both RGB and depth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955a781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[WARN ] Dependency `gsplat` is required for rendering 3DGS. Install via: pip install git+https://github.com/nerfstudio-project/gsplat.git@0b4dddf04cb687367602c01196913cde6a743d70\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add Depth-Anything-3 src to Python path\n",
    "DA3_SRC = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'Depth-Anything-3', 'src'))\n",
    "sys.path.insert(0, DA3_SRC)\n",
    "\n",
    "# Repo root (two levels up from analysis/tutorials/)\n",
    "ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from depth_anything_3.api import DepthAnything3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10060867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"depth-anything/DA3-SMALL\"):\n",
    "    \"\"\"Load a Depth Anything V3 model onto GPU (or CPU fallback).\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading model: {model_name} on {device} ...\")\n",
    "    model = DepthAnything3.from_pretrained(model_name).to(device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e9f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, sample_rate=30):\n",
    "    \"\"\"Extract every Nth frame from a video. Returns a list of (frame_index, BGR numpy array).\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Video: {total_frames} frames, {fps:.1f} fps, sampling every {sample_rate} frames\")\n",
    "\n",
    "    frames = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % sample_rate == 0:\n",
    "            frames.append((idx, frame))\n",
    "        idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(frames)} frames\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95c7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(model, video_path, output_dir, sample_rate=30, cmap=\"inferno\"):\n",
    "    \"\"\"\n",
    "    Full pipeline: extract frames, run depth, save RGB + depth images.\n",
    "    \n",
    "    Output structure:\n",
    "        <output_dir>/<video_name>-rgb/frame_000001.png, ...\n",
    "        <output_dir>/<video_name>-d/frame_000001.png, ...\n",
    "    \"\"\"\n",
    "    video_name = Path(video_path).stem\n",
    "\n",
    "    rgb_dir = os.path.join(output_dir, f\"{video_name}-rgb\")\n",
    "    depth_dir = os.path.join(output_dir, f\"{video_name}-d\")\n",
    "    os.makedirs(rgb_dir, exist_ok=True)\n",
    "    os.makedirs(depth_dir, exist_ok=True)\n",
    "\n",
    "    frames = extract_frames(video_path, sample_rate)\n",
    "\n",
    "    for i, (frame_idx, bgr_frame) in enumerate(frames):\n",
    "        fname = f\"frame_{i+1:06d}.png\"\n",
    "\n",
    "        # Save RGB frame (convert BGR -> RGB for saving)\n",
    "        rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_path = os.path.join(rgb_dir, fname)\n",
    "        plt.imsave(rgb_path, rgb_frame)\n",
    "\n",
    "        # Run depth estimation (DA3 accepts numpy RGB arrays)\n",
    "        prediction = model.inference([rgb_frame])\n",
    "        depth = prediction.depth[0]  # [H, W] float32\n",
    "\n",
    "        # Save depth map\n",
    "        depth_path = os.path.join(depth_dir, fname)\n",
    "        plt.imsave(depth_path, depth, cmap=cmap)\n",
    "\n",
    "        print(f\"  [{i+1}/{len(frames)}] video frame {frame_idx} -> {fname}\")\n",
    "\n",
    "    print(f\"\\nDone! RGB: {rgb_dir}  |  Depth: {depth_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff38a9",
   "metadata": {},
   "source": [
    "# Run Video Depth Estimation\n",
    "Set your input video, output directory, and sample rate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef13a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: depth-anything/DA3-SMALL on cuda ...\n",
      "\u001b[97m[INFO ] using MLP layer as FFN\u001b[0m\n",
      "Video: 1638 frames, 24.0 fps, sampling every 30 frames\n",
      "Extracted 55 frames\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.026705503463745117 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.46265339851379395 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0011136531829833984 seconds\u001b[0m\n",
      "  [1/55] video frame 0 -> frame_000001.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.026996850967407227 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.11876988410949707 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004761219024658203 seconds\u001b[0m\n",
      "  [2/55] video frame 30 -> frame_000002.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03297281265258789 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.0325932502746582 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004241466522216797 seconds\u001b[0m\n",
      "  [3/55] video frame 60 -> frame_000003.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03151345252990723 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03232216835021973 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004177093505859375 seconds\u001b[0m\n",
      "  [4/55] video frame 90 -> frame_000004.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03467106819152832 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03212308883666992 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004038810729980469 seconds\u001b[0m\n",
      "  [5/55] video frame 120 -> frame_000005.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.023624897003173828 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032123565673828125 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040268898010253906 seconds\u001b[0m\n",
      "  [6/55] video frame 150 -> frame_000006.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.030168771743774414 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03273916244506836 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004210472106933594 seconds\u001b[0m\n",
      "  [7/55] video frame 180 -> frame_000007.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03239321708679199 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.0317997932434082 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004291534423828125 seconds\u001b[0m\n",
      "  [8/55] video frame 210 -> frame_000008.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03001260757446289 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03272843360900879 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004074573516845703 seconds\u001b[0m\n",
      "  [9/55] video frame 240 -> frame_000009.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.029208898544311523 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03184080123901367 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041031837463378906 seconds\u001b[0m\n",
      "  [10/55] video frame 270 -> frame_000010.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03980064392089844 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032486677169799805 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040793418884277344 seconds\u001b[0m\n",
      "  [11/55] video frame 300 -> frame_000011.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.030290603637695312 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03194379806518555 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003979206085205078 seconds\u001b[0m\n",
      "  [12/55] video frame 330 -> frame_000012.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.038309335708618164 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03176546096801758 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004010200500488281 seconds\u001b[0m\n",
      "  [13/55] video frame 360 -> frame_000013.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03594350814819336 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.031684160232543945 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004115104675292969 seconds\u001b[0m\n",
      "  [14/55] video frame 390 -> frame_000014.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02853083610534668 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032616376876831055 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041294097900390625 seconds\u001b[0m\n",
      "  [15/55] video frame 420 -> frame_000015.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.030028820037841797 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03188800811767578 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040793418884277344 seconds\u001b[0m\n",
      "  [16/55] video frame 450 -> frame_000016.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03329968452453613 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03205394744873047 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004184246063232422 seconds\u001b[0m\n",
      "  [17/55] video frame 480 -> frame_000017.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02656412124633789 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03171801567077637 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004055500030517578 seconds\u001b[0m\n",
      "  [18/55] video frame 510 -> frame_000018.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02301335334777832 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03209972381591797 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004253387451171875 seconds\u001b[0m\n",
      "  [19/55] video frame 540 -> frame_000019.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.025197505950927734 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.0324246883392334 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004260540008544922 seconds\u001b[0m\n",
      "  [20/55] video frame 570 -> frame_000020.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02658390998840332 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03263568878173828 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040793418884277344 seconds\u001b[0m\n",
      "  [21/55] video frame 600 -> frame_000021.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.025650978088378906 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03242182731628418 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040531158447265625 seconds\u001b[0m\n",
      "  [22/55] video frame 630 -> frame_000022.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02159881591796875 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032227277755737305 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004127025604248047 seconds\u001b[0m\n",
      "  [23/55] video frame 660 -> frame_000023.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.023566722869873047 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03268289566040039 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040984153747558594 seconds\u001b[0m\n",
      "  [24/55] video frame 690 -> frame_000024.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02656269073486328 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.031832218170166016 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00039839744567871094 seconds\u001b[0m\n",
      "  [25/55] video frame 720 -> frame_000025.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.022809982299804688 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03174233436584473 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.000408172607421875 seconds\u001b[0m\n",
      "  [26/55] video frame 750 -> frame_000026.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.021026134490966797 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03269600868225098 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00047135353088378906 seconds\u001b[0m\n",
      "  [27/55] video frame 780 -> frame_000027.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.033237457275390625 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03192782402038574 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040340423583984375 seconds\u001b[0m\n",
      "  [28/55] video frame 810 -> frame_000028.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.027215003967285156 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03270721435546875 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004115104675292969 seconds\u001b[0m\n",
      "  [29/55] video frame 840 -> frame_000029.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.030843496322631836 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03165721893310547 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004134178161621094 seconds\u001b[0m\n",
      "  [30/55] video frame 870 -> frame_000030.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.029636859893798828 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03246593475341797 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003986358642578125 seconds\u001b[0m\n",
      "  [31/55] video frame 900 -> frame_000031.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.032560110092163086 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.031966447830200195 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.000400543212890625 seconds\u001b[0m\n",
      "  [32/55] video frame 930 -> frame_000032.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.028717756271362305 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03215646743774414 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041985511779785156 seconds\u001b[0m\n",
      "  [33/55] video frame 960 -> frame_000033.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.029039621353149414 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032988786697387695 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00042247772216796875 seconds\u001b[0m\n",
      "  [34/55] video frame 990 -> frame_000034.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.032434701919555664 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03242969512939453 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041747093200683594 seconds\u001b[0m\n",
      "  [35/55] video frame 1020 -> frame_000035.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.028019428253173828 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03296494483947754 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004076957702636719 seconds\u001b[0m\n",
      "  [36/55] video frame 1050 -> frame_000036.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.030895709991455078 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.031777381896972656 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004086494445800781 seconds\u001b[0m\n",
      "  [37/55] video frame 1080 -> frame_000037.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.022632837295532227 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03170180320739746 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.000438690185546875 seconds\u001b[0m\n",
      "  [38/55] video frame 1110 -> frame_000038.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02469468116760254 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03204154968261719 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003981590270996094 seconds\u001b[0m\n",
      "  [39/55] video frame 1140 -> frame_000039.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03206610679626465 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.031900882720947266 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040221214294433594 seconds\u001b[0m\n",
      "  [40/55] video frame 1170 -> frame_000040.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.029497861862182617 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03184771537780762 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041961669921875 seconds\u001b[0m\n",
      "  [41/55] video frame 1200 -> frame_000041.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03251481056213379 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03187680244445801 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00039887428283691406 seconds\u001b[0m\n",
      "  [42/55] video frame 1230 -> frame_000042.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.029203176498413086 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03189444541931152 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040340423583984375 seconds\u001b[0m\n",
      "  [43/55] video frame 1260 -> frame_000043.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.0295870304107666 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03175473213195801 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00041556358337402344 seconds\u001b[0m\n",
      "  [44/55] video frame 1290 -> frame_000044.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.025079011917114258 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03179740905761719 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004246234893798828 seconds\u001b[0m\n",
      "  [45/55] video frame 1320 -> frame_000045.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02207660675048828 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03277945518493652 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004088878631591797 seconds\u001b[0m\n",
      "  [46/55] video frame 1350 -> frame_000046.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.026303768157958984 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.032411813735961914 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003979206085205078 seconds\u001b[0m\n",
      "  [47/55] video frame 1380 -> frame_000047.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.026778221130371094 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03256678581237793 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00040841102600097656 seconds\u001b[0m\n",
      "  [48/55] video frame 1410 -> frame_000048.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.026505470275878906 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03224015235900879 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003998279571533203 seconds\u001b[0m\n",
      "  [49/55] video frame 1440 -> frame_000049.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.025959014892578125 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03184628486633301 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004105567932128906 seconds\u001b[0m\n",
      "  [50/55] video frame 1470 -> frame_000050.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.028084754943847656 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.0319676399230957 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004134178161621094 seconds\u001b[0m\n",
      "  [51/55] video frame 1500 -> frame_000051.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02240920066833496 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03228044509887695 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.000396728515625 seconds\u001b[0m\n",
      "  [52/55] video frame 1530 -> frame_000052.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.03146982192993164 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03228020668029785 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0003974437713623047 seconds\u001b[0m\n",
      "  [53/55] video frame 1560 -> frame_000053.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.027887344360351562 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03221273422241211 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0004258155822753906 seconds\u001b[0m\n",
      "  [54/55] video frame 1590 -> frame_000054.png\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.02560710906982422 seconds. Shape:  torch.Size([1, 3, 280, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.03199505805969238 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.00039649009704589844 seconds\u001b[0m\n",
      "  [55/55] video frame 1620 -> frame_000055.png\n",
      "\n",
      "Done! RGB: /home/navlab/sukeerth/Tree/TreeHacks/treehacks-2026/analysis/outputs/depth/surgery_video-rgb  |  Depth: /home/navlab/sukeerth/Tree/TreeHacks/treehacks-2026/analysis/outputs/depth/surgery_video-d\n"
     ]
    }
   ],
   "source": [
    "# ---- Hyperparameters (paths relative to repo root) ----\n",
    "input_video_path = \"analysis/data/depth/surgery_video.mp4\"\n",
    "output_dir       = \"analysis/outputs/depth/\"\n",
    "sample_rate      = 30     # sample every Nth frame\n",
    "model_name       = \"depth-anything/DA3-SMALL\"\n",
    "# --------------------------------------------------------\n",
    "\n",
    "input_video_path = os.path.join(ROOT, input_video_path)\n",
    "output_dir       = os.path.join(ROOT, output_dir)\n",
    "\n",
    "model = load_model(model_name)\n",
    "process_video(model, input_video_path, output_dir, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106dac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navlab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
